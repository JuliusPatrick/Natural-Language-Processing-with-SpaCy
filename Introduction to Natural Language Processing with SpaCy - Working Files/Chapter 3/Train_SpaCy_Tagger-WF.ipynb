{"nbformat": 4, "cells": [{"source": "<h3>Basic Recipe for Training a POS Tagger with SpaCy</h3>\n<ol>\n<li id=\"loaddatatitle\"><a href=\"#-Load-Data-\">Load Data </a>\n<ol><li>We'll be using a sample from Web Treebank corpus, in ConllX format</ol>\n<li><a href=\"#Prepare-Environment-for-New-Model\">Prepare environment for a new model</a>\n<ol><li>New model directory, with tagger and parser subdirectories. (Ensure you have permission)</ol>\n<li><a href=\"#Build-a-Vocabulary\">Build a vocabulary</a>\n\n<ol>\n<li>We are just going to load the default English Vocabulary\n<li>Defines how we get attributes (like suffix) from a token string\n<li>Includes brown cluster data on lexemes, we'll use as a feature for the parser\n</ol>\n<li> <a href=\"#Build-a-Tagger\">Build a Tagger</a>\n<ol><li>Ensure tagmap is provided if needed</ol>\n<ol><li>Which features should be used to train tagger?</ol>\n<li><a href=\"#Train-Tagger\"> Train Tagger</a>\n<ol><li>Averaged Perceptron algorithm\n<li>For each epoch: \n<ol><li>For each document in training data:\n<ol><li>For each sentence in document:\n<ol>\n    <li>Create document with sentence words (tagger not yet applied)\n    <li>Create GoldParse object with annotated labels\n    <li>Apply the tagger to the document to get predictions\n    <li>Update the tagger with GoldParse, Document (actual v predicted)\n</ol>\n</ol>\n<li> Score predictions on validation set\n</ol>\n</ol>\n<li><a href=\"#Save-Tagger\">Save Tagger</a>", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"source": "<h3> Load Data </h3>", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "import sys\nsys.path.append('/home/jupyter/site-packages/')", "execution_count": 2, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [{"output_type": "stream", "name": "stdout", "text": "Skipped 202 malformed lines\nSkipped 1 malformed lines\n"}], "source": "import requests\nfrom spacy.syntax.arc_eager import PseudoProjectivity\ndef sent_iter(conll_corpus):\n    for _, doc_sents in conll_corpus:\n        for (ids, words, tags, heads, deps, ner), _ in doc_sents:\n            yield ids, words, tags, heads, deps, ner\n            \ndef read_conllx(text):\n    bad_lines = 0\n    for sent in text.strip().split('\\n\\n'):       \n        lines = sent.strip().split('\\n')\n        if lines:\n            while lines[0].startswith('#'):\n                lines.pop(0)\n            tokens = []\n            for line in lines:\n                try:\n                    id_, word, lemma, tag, pos, morph, head, dep, _1, _2 = line.split()\n                    if '-' in id_:\n                        continue\n                    id_ = int(id_) - 1\n                    head = (int(head) - 1) if head != '0' else id_\n                    dep = 'ROOT' if dep == 'root' else dep\n                    tokens.append((int(id_), unicode(word), unicode(pos), int(head), unicode(dep), 'O'))\n                except:\n                    bad_lines += 1\n            tuples = [list(t) for t in zip(*tokens)]\n            yield (None, [[tuples, []]])\n    print \"Skipped {} malformed lines\".format(bad_lines)\n\n            \n            \ndef LoadData(url, make_projective = False):\n    conll_string = requests.get(url).content\n    sents = list(read_conllx(conll_string))\n    if make_projective:\n        sents = PseudoProjectivity.preprocess_training_data(sents)\n    return sents\n    \n    \ntrain_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-train.conllu'\ntest_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-test.conllu'\ntrain_sents = LoadData(train_url)\ntest_sents = LoadData(test_url)", "execution_count": 12, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [{"output_type": "stream", "name": "stdout", "text": "Training corpus metadata\n\nNumber of Sentences: 12543\nNumber of Unique Tags: 50\nUnique Tags: set([u'PRP$', u'VBG', u'VBD', u'NFP', u'``', u'VBN', u'POS', u\"''\", u'VBP', u'WDT', u'JJ', u'WP', u'VBZ', u'DT', u'RP', u'$', u'NN', u'FW', u',', u'.', u'TO', u'PRP', u'RB', u'-LRB-', u':', u'NNS', u'HYPH', u'GW', u'XX', u'VB', u'WRB', u'CC', u'LS', u'PDT', u'RBS', u'RBR', u'CD', u'ADD', u'AFX', u'EX', u'IN', u'WP$', u'MD', u'NNPS', u'-RRB-', u'JJS', u'JJR', u'SYM', u'UH', u'NNP'])\n"}], "source": "sent_counter = 0\nunique_tags = set()\nfor ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n    unique_tags.update(tags)\n    sent_counter += 1\ndoc_counter = len(train_sents)\nprint \"Training corpus metadata\"\nprint\nprint \"Number of Sentences: {}\".format(sent_counter)\nprint \"Number of Unique Tags: {}\".format(len(unique_tags))\nprint \"Unique Tags: {}\".format(unique_tags)", "execution_count": 13, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Prepare Environment for New Model", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "from pathlib import Path\nimport spacy\n\ndef prepare_environment_for_new_tagger(model_path, tagger_path):\n    if not model_dir.exists():\n        model_dir.mkdir()\n    if not tagger_path.exists():\n        tagger_path.mkdir()\n        \ndata_dir = spacy.en.get_data_path()\nmodel_dir = data_dir / 'en-1.1.0'\ntagger_dir = model_dir / 'custom-pos-tagger'\nprepare_environment_for_new_tagger(model_dir, tagger_dir)", "execution_count": 14, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Build a Vocabulary", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "from spacy.vocab import Vocab\ndef build_vocab(model_dir, vec_path = None, lexeme_path = None):\n    vocab = Vocab.load(model_dir)\n    if lexeme_path:\n        vocab.load_lexemes(lexeme_path)\n    if vec_path:\n        vocab.load_vectors_from_bin_loc(vec_path)\n        \n    return vocab\n    \nlexeme_path = model_dir / 'vocab' / 'lexemes.bin'\nvocab = build_vocab(model_dir, lexeme_path=lexeme_path)", "execution_count": 15, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [{"output_type": "stream", "name": "stdout", "text": "Cluster Value for 'He': 126\n"}], "source": "#test clusters are available\nfrom spacy.tokens import Doc\n\ndoc = Doc(vocab, words=[u'He',u'ate',u'pizza',u'.'])\nprint \"Cluster Value for '{}': {}\".format(*[doc[0], doc[0].cluster])", "execution_count": 6, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Build a Tagger", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "from spacy.tagger import Tagger\nfrom spacy.tagger import *\n\nfeatures = [\n    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),    #current word attributes   \n    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),                        #-1 word attributes \n    (P2_pos,),(P2_cluster,),(P2_flags,),                                     #-2 word attributes  \n    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),                       #+1 word attributes       \n    (N2_orth,),(N2_cluster,),(N2_flags,),                                    #+2 word attributes \n    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth) #combination attributes \n]\n\nfeatures = spacy.en.English.Defaults.tagger_features\ntag_map = spacy.en.tag_map\nstatistical_model = spacy.tagger.TaggerModel(features)\ntagger = Tagger(vocab, tag_map=tag_map, statistical_model = statistical_model)", "execution_count": 7, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Train Tagger", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [{"output_type": "stream", "name": "stdout", "text": "Iteration:\t\tPOS Tag Accuracy\nPretraining:\t\t0.000\n0:\t\t\t87.655\n1:\t\t\t89.122\n2:\t\t\t91.250\n3:\t\t\t91.110\n4:\t\t\t91.453\n5:\t\t\t91.851\n6:\t\t\t92.545\n7:\t\t\t92.302\n8:\t\t\t92.246\n9:\t\t\t91.843\n"}], "source": "from spacy.scorer import Scorer\nfrom spacy.gold import GoldParse\nimport random\n\n\ndef score_model(vocab, tagger, gold_docs, verbose=False):\n    scorer = Scorer()\n    for _, gold_doc in gold_docs:\n        for (ids, words, tags, heads, deps, entities), _ in gold_doc:\n            doc = Doc(vocab, words=map(unicode,words))\n            tagger(doc)\n            gold = GoldParse(doc, tags=tags)\n            scorer.score(doc, gold, verbose=verbose)\n    return scorer  \n\n\ndef train(tagger, vocab, train_sents, test_sents, model_dir, n_iter=20, seed = 0, feat_set = u'basic'):\n    scorer = score_model(vocab, tagger, test_sents)\n    print('%s:\\t\\t%s' % (\"Iteration\", \"POS Tag Accuracy\"))            \n    print('%s:\\t\\t%.3f' % (\"Pretraining\", scorer.tags_acc))        \n    \n    #TRAINING STARTS HERE\n    for itn in range(n_iter):\n        for ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n            doc = Doc(vocab, words=map(unicode,words))\n            gold = GoldParse(doc, tags=tags, heads=heads, deps=deps)\n            tagger(doc)\n            tagger.update(doc, gold)\n        random.shuffle(train_sents)\n        scorer = score_model(vocab, tagger, test_sents)\n        print('%d:\\t\\t\\t%.3f' % (itn, scorer.tags_acc))\n    return tagger\ntrained_tagger = train(tagger, vocab, train_sents, test_sents, model_dir, n_iter = 10)", "execution_count": 9, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "<a href=\"#loaddatatitle\">back</a>\n<br>\n### Save Tagger", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "def ensure_dir(path):\n    if not path.exists():\n        path.mkdir()\n        \nensure_dir(tagger_dir)\ntrained_tagger.model.dump(str(tagger_dir / 'model'))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"source": "### Notes\n<br>\n1. Spacy will be rolling out a neural network model soon!\n<br>\n<br>\n2. Checkout Speech and Language Processing by Daniel Jurafsky and James H. Martin\n<br>\n<br>\n3. Next section: Vector space models for natural language.", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "codemirror_mode": {"name": "ipython", "version": 2}, "version": "2.7.12", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python"}, "_datascience": {"notebookId": 750}, "kernelspec": {"language": "python", "name": "python2", "display_name": "Python 2"}}, "nbformat_minor": 2}