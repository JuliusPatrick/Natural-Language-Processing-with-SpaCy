{"nbformat": 4, "cells": [{"outputs": [], "source": "# !pip install spacy\n# !pip install nltk\n# !pip install plotly\n# !pip install cufflinks\n# !python -m spacy.en.download", "execution_count": 1, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "# import sys\n# sys.path.append('/home/jupyter/site-packages/')", "execution_count": 3, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [{"output_type": "error", "ename": "ImportError", "evalue": "No module named nltk", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-4-f7cf44040934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mImportError\u001b[0m: No module named nltk"]}], "source": "import pandas as pd\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS\nfrom spacy.en import English\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport nltk\nfrom operator import itemgetter\nfrom itertools import groupby\nfrom nltk.corpus import brown\nfrom collections import defaultdict, Counter\nimport numpy as np\nfrom spacy.tokens import Doc\nfrom IPython.display import HTML\nimport warnings\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.offline as offline\nimport cufflinks as cf\nimport pandas as pd\n\n\ncf.go_offline()\noffline.init_notebook_mode()\nPLOT_API_KEY = os.environ['SECRET_ENV_AARON_PLOT_API_KEY']\nPLOTLY_USERNAME = os.environ['SECRET_ENV_AARON_PLOTLY_USERNAME']\npy.sign_in(PLOTLY_USERNAME,PLOT_API_KEY)\n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndef rep_sentences(texts):\n    html = []\n    for text in texts:\n        html.append(rep_sentence(text))\n    return HTML(\"\".join(html))\n\ndef rep_sentence(text, display_pos = True):\n    html_colors = ['SkyBlue'\n               ,'red'\n               ,'YellowGreen'\n               ,'yellow'\n               ,'orange'\n               ,'pink'\n               ,'brown'\n               ,'purple'\n               , 'CadetBlue'\n                ,'DarkKhaki'\n                ,'DarkSalmon'\n                ,'Gold'    \n              ]\n    doc = nlp(unicode(text))\n    n_words = len(doc)\n    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n    css = [\"<style>.word{font-weight:bold;}</style>\"]\n    for pos in unique_pos:\n        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n    css = \"\".join(css)\n\n    html = [\"<table width=100%>\"]\n    html.append(css)\n    html.append(\"<tr>\")            \n    for i in xrange(n_words):\n        word_string= doc[i].orth_\n        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n    html.append(\"</tr>\")\n    if display_pos:\n        html.append(\"<tr>\")            \n        for i in xrange(n_words):\n            pos = doc[i].pos_\n            color = pos_to_color[pos]\n            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n        html.append(\"</tr>\")\n    html = \"\".join(html)\n    return html\n\n\n\ndef custom_tag_table(list_of_word_tag_tuples):\n    html_colors = ['SkyBlue'\n               ,'red'\n               ,'YellowGreen'\n               ,'yellow'\n               ,'orange'\n               ,'pink'\n               ,'brown'\n               ,'MediumPurple'\n               , 'CadetBlue'\n                ,'DarkKhaki'\n                ,'DarkSalmon'\n                ,'Gold'    \n              ]\n    \n    n_words = len(list_of_word_tag_tuples)\n    words, pos_list = zip(*list_of_word_tag_tuples)\n    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n    css = [\"<style>.word{font-weight:bold;}</style>\"]\n    for pos in unique_pos:\n        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n    css = \"\".join(css)\n\n    html = [\"<table width=100%>\"]\n    html.append(css)\n    for i in xrange(n_words):\n        html.append(\"<tr>\")            \n        word_string= words[i]\n        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n        row = []\n        pos_sublist = pos_list[i]\n        for pos in pos_sublist:\n            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n            #print entry\n            row.append(entry)\n        row = \"\".join(row)\n        html.append(\"<td>{}</td>\".format(row))\n        html.append(\"</tr>\")\n    return \"\".join(html)\n        \n    \n\ndef nltk_corpus(corpus_name):\n    corpus = getattr(nltk.corpus, corpus_name)\n    try:\n        corpus.ensure_loaded()\n    except:\n        nltk.download(corpus_name)\n    return corpus\n\n#read nltk corpora\ndef nltk_reader(corpus_name, limit = None):\n    corpus = nltk_corpus(corpus_name)\n    fileids = corpus.fileids()\n    \n    if limit:\n        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n    else:\n        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n    return doc_iter\n\nuniversal_tags = [\n     ['Open Class Words','ADJ','adjective']\n    ,['Open Class Words','ADV','adverb']\n    ,['Open Class Words','INTJ','interjection']\n    ,['Open Class Words','NOUN','noun']\n    ,['Open Class Words','PROPN','proper noun']\n    ,['Open Class Words','VERB','verb']\n    ,['Closed Class Words','ADP','adposition']\n    ,['Closed Class Words','AUX','auxiliary']\n    ,['Closed Class Words','CCONJ','coordination conjunction']\n    ,['Closed Class Words','DET','determiner']\n    ,['Closed Class Words','NUM','numeral']\n    ,['Closed Class Words','PART','particle']\n    ,['Closed Class Words','PRON','pronoun']\n    ,['Closed Class Words','SCONJ','subordinating conjection']\n    ,['Other','PUNCT','punctuation']\n    ,['Other','SYM','symbol']\n    ,['Other','X','other']\n]\ntag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech'])\ntag_table = tag_table.set_index(['Category','Abbrev'])\n\nnltk.download('tagsets')\nnltk.download('universal_tagset')\nnlp = spacy.load('en', parser = False,entity=False)", "execution_count": 4, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "### POS Tagsets\n\n##### Universal: token.pos_", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "tag_table", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "##### Penn: token.tag_", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"source": "### Part of Speech Tags for Disambiguation", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [{"output_type": "execute_result", "data": {"text/html": "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.DET{background-color:yellow;}</style><style>.PRON{background-color:orange;}</style><style>.VERB{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.PRON{background-color:SkyBlue;}</style><style>.VERB{background-color:red;}</style><style>.NOUN{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>", "text/plain": "<IPython.core.display.HTML object>"}, "execution_count": 22, "metadata": {}}], "source": "sentence1 = 'I get a discount on newspapers.'\nsentence2 = 'I discount that newspaper.'\n\nrep_sentences([sentence1, sentence2])", "execution_count": 22, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [{"output_type": "display_data", "data": {"text/html": "<div id=\"65d04935-b01f-48bf-81ab-037df879d925\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"65d04935-b01f-48bf-81ab-037df879d925\", [{\"name\": \"None\", \"text\": \"\", \"y\": [0.5501011898853854, 0.3026099083655352, 0.08579691030185371, 0.05082373445351574, 0.009092790199637634, 0.0006863752824148297, 0.0008890915116574449], \"marker\": {\"color\": \"rgba(255, 153, 51, 0.6)\", \"line\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"width\": 1}}, \"x\": [1, 2, 3, 4, 5, 6, 7], \"type\": \"bar\", \"orientation\": \"v\"}], {\"title\": \"45% of Brown Corpus of Spacy-tagged tokens are ambiguous\", \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"xaxis1\": {\"tickfont\": {\"color\": \"#4D5663\"}, \"title\": \"Unique Parts of Speech in Brown Corpus\", \"showgrid\": true, \"zerolinecolor\": \"#E1E5ED\", \"gridcolor\": \"#E1E5ED\", \"titlefont\": {\"color\": \"#4D5663\"}}, \"yaxis1\": {\"tickfont\": {\"color\": \"#4D5663\"}, \"title\": \"Percent of Unique Words in Vocabulary\", \"showgrid\": true, \"zerolinecolor\": \"#E1E5ED\", \"gridcolor\": \"#E1E5ED\", \"titlefont\": {\"color\": \"#4D5663\"}}, \"titlefont\": {\"color\": \"#4D5663\"}, \"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>", "text/plain": "<IPython.core.display.HTML object>"}, "metadata": {}}], "source": "# #this example uses shows the frequency of tokens with multiple\n# #parts of speech\n\n\n# #create a dictionary of sets\n# unique_tag_dictionary = defaultdict(set)\n\n# for doc in nlp.pipe(nltk_reader('brown'), n_threads=4):\n#     for token in doc:\n#         #add the token's part of speech to the lexeme's set\n#         unique_tag_dictionary[token.orth_].add(token.pos_)\n        \n# #for each word, how many unique POS's were found?\n# n_word_senses = {word: len(senses) for word, senses in unique_tag_dictionary.items()}\n\n# #map each word to its number of senses, and count\n# ambiguous_word_counts = Counter()\n# for doc in nlp.pipe(nltk_reader('brown'), n_threads=4):\n#     word_senses = map(lambda token: n_word_senses[token.orth_] , doc)\n#     ambiguous_word_counts.update(word_senses)\n\n# #normalize and feed to pandas    \n# N = float(sum(ambiguous_word_counts.values()))\n# plot_data = pd.Series(ambiguous_word_counts).map(lambda x: x / N)\n\n#plot\nplot_data.iplot(kind='bar'\n                , title = \"45% of Brown Corpus of Spacy-tagged tokens are ambiguous\"\n                , xTitle = \"Unique Parts of Speech in Brown Corpus\"\n                , yTitle= \"Percent of Unique Words in Vocabulary\"\n               )", "execution_count": 21, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "#pick first sentence\nsent = list(doc.sents)[0]\n\n#make a list of (word, unique_tags) pairs\nword_tag_list = [(token.orth_, list(unique_tag_dictionary[token.lemma_])) for token in sent]\n\n#display\nHTML(custom_tag_table(word_tag_list))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "### Inferring Parts of Speech", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [{"output_type": "execute_result", "data": {"text/html": "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.PROPN{background-color:yellow;}</style><style>.DET{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PART{background-color:brown;}</style><style>.ADJ{background-color:purple;}</style><style>.VERB{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>", "text/plain": "<IPython.core.display.HTML object>"}, "execution_count": 16, "metadata": {}}], "source": "text = u'I was loble to find the effix by klepping the Dongle search engine.'\ndocument = nlp(text)\nHTML(rep_sentence(document, display_pos = False))", "execution_count": 16, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [{"output_type": "execute_result", "data": {"text/html": "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.ADP{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.PROPN{background-color:yellow;}</style><style>.DET{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PART{background-color:brown;}</style><style>.ADJ{background-color:purple;}</style><style>.VERB{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADJ'>ADJ</span></td><td><span class='PART'>PART</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='PROPN'>PROPN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>", "text/plain": "<IPython.core.display.HTML object>"}, "execution_count": 17, "metadata": {}}], "source": "HTML(rep_sentence(document, display_pos = True))", "execution_count": 17, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"source": "### Determinants of Part of Speech:\n* Word: some words can only be used in a single way; we can memorize these.\n* Word shape: if the first letter is capitalized, its likely a proper noun.\n* Neighboring part of speech: there are common patterns, such as noun phrases commonly following a determiner. to the beach\n\n\n\n| Feature | Notes | Example|\n|------|------|------|\n|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n|?|?|?|\n", "cell_type": "markdown", "metadata": {"_datascience": {}}}, {"outputs": [], "source": "from sklearn.cross_validation import train_test_split\ncorpus = nltk_corpus('brown')\nall_data = np.array(corpus.tagged_sents(tagset='brown'))\nall_data = [zip(*i) for i in all_data]\ntrain, test = train_test_split(all_data, test_size = .1)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "#EXPERIMENTAL\nfrom spacy.symbols import *\nfrom spacy.language_data import TAG_MAP\nfrom spacy.vocab import Vocab\nfrom spacy.tagger import Tagger\nfrom spacy.tokens import Doc\nfrom spacy.gold import GoldParse\nfrom nltk.tag import tagset_mapping\nfrom spacy.en import English\nfrom spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n\ndef validate(test_data, tagger):\n    correct = 0\n    total = 0\n    tagmap = tagger.vocab.morphology.tag_map\n    for words, tags in test_data:\n        doc = Doc(vocab, words=words)\n        tagger(doc)\n        predictions = map(lambda token: tagmap[token.tag_], doc)\n        actual = map(lambda tag: tagmap[tag], tags)\n        \n        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n        n_correct = len(correct_predictions)\n        correct += n_correct\n        total += len(words)\n        \n    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n    return result\n\ndef generate_tagmap():\n    def adjust_value(x):\n        if x == '.':\n            val = PUNCT\n        elif x=='PRT':\n            val = PART\n        else:\n            val = getattr(spacy.symbols, x)\n        return {POS:val}\n    nltk_map = tagset_mapping('en-brown','universal')\n    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n    return adj_map\n\nfeatures = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n,(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n,(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n,(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n,(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n\ntagmap = generate_tagmap()\ngold_tagmap = nlp.vocab.morphology.tag_map\nfor key in tagmap:\n    gold_tagmap[key] = tagmap[key]\n\nvocab = nlp.vocab\ntagger = nlp.tagger\n\n\npretraining_accuracy = validate(test, tagger)\nprint pretraining_accuracy\n\nfor i in range(10):\n    train_fold, test_fold = train_test_split(train, test_size = .2)\n    for words, tags in train_fold:\n        doc = Doc(vocab, words=words)\n        gold = GoldParse(doc, tags=tags)   \n        tagger.update(doc, gold)\n    current_accuracy = validate(test_fold, tagger)\n    print current_accuracy\n    np.random.shuffle(train)\ntagger.model.end_training()\n\nposttraining_accuracy = validate(test, tagger)\nprint posttraining_accuracy", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.symbols import *\nfrom spacy.language_data import TAG_MAP\nfrom spacy.vocab import Vocab\nfrom spacy.tagger import Tagger\nfrom spacy.tokens import Doc\nfrom spacy.gold import GoldParse\nfrom nltk.tag import tagset_mapping\nfrom spacy.language import Language\nfrom spacy.tagger import Tagger\nfrom spacy.scorer import Scorer\nfrom spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n\ndef validate(test_data, tagger):\n    correct = 0\n    total = 0\n    tagmap = tagger.vocab.morphology.tag_map\n    for words, tags in test_data:\n        doc = Doc(vocab, words=words)\n        tagger(doc)\n        predictions = map(lambda token: tagmap[token.tag_], doc)\n        actual = map(lambda tag: tagmap[tag], tags)\n        \n        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n        n_correct = len(correct_predictions)\n        correct += n_correct\n        total += len(words)\n        \n    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n    return result\n\ndef generate_tagmap():\n    def adjust_value(x):\n        if x == '.':\n            val = PUNCT\n        elif x=='PRT':\n            val = PART\n        else:\n            val = getattr(spacy.symbols, x)\n        return {POS:val}\n    nltk_map = tagset_mapping('en-brown','universal')\n    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n    return adj_map\n\nN_ITER = 1\n\n\ndef make_tagger(vocab, templates):\n    model = spacy.tagger.TaggerModel(templates)\n    return spacy.tagger.Tagger(vocab,model)\n\n\n\ntagmap = generate_tagmap()\nvocab = Vocab(tag_map = tagmap)\ntagger = make_tagger(vocab, nlp.tagger.feature_templates)\n\n\nprint(\"Itn.\\ttrain acc %\\tdev acc %\")\nfor itn in xrange(N_ITER):\n    print \"Iteration {}\".format(itn) \n    scorer = Scorer()\n    COUNTER = 0\n    for words, tags in train[:100]:\n        words = map(unicode, words)\n        doc = Doc(vocab, words = map(unicode, words))\n        gold = GoldParse(doc, tags=tags)   \n        tagger.update(doc, gold)\n        scorer.score(doc, gold)\n        COUNTER += 1\n        if COUNTER % 1000 == 0:\n            print COUNTER, correct/total    \n        \n    train_acc = correct/total\n    np.random.shuffle(train)\n    print train_acc", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "print(\"Itn.\\ttrain acc %\\tdev acc %\")\nfor itn in xrange(10):\n    print \"Iteration {}\".format(itn) \n    scorer = Scorer()\n    COUNTER = 0\n    for words, tags in train[:10000]:\n        words = map(unicode, words)\n        doc = Doc(vocab, words = map(unicode, words))\n        gold = GoldParse(doc, tags=tags)   \n        tagger.update(doc, gold)\n        scorer.score(doc, gold)\n    print scorer.tags_acc\n    np.random.shuffle(train)\n", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "for itn in range(5):\n    print \"Iteration {}\".format(itn) \n    correct, total = 0., 0.\n    COUNTER = 0\n    for words, tags in train[10000:20000]:\n        words = map(unicode, words)\n        doc = Doc(vocab, words = map(unicode, words))\n        gold = GoldParse(doc, tags=tags)   \n        correct += tagger.update(doc, gold)\n        total += len(words)\n        COUNTER += 1\n        if COUNTER % 1000 == 0:\n            print COUNTER, correct/total    \n    train_acc = correct/total\n    np.random.shuffle(train)\n    print train_acc", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "tagger.update(doc, gold)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "doc = nlp.tokenizer(\" \".join(map(unicode,words)))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "TaggerModel.update", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "from spacy.symbols import *\nfrom spacy.language_data import TAG_MAP\nfrom spacy.vocab import Vocab\nfrom spacy.tagger import Tagger\nfrom spacy.tokens import Doc\nfrom spacy.gold import GoldParse\nfrom nltk.tag import tagset_mapping\nfrom spacy.en import English\nfrom spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n\ndef validate(test_data, tagger):\n    correct = 0\n    total = 0\n    \n    for words, tags in test_data:\n        doc = Doc(vocab, words=words)\n        tagger(doc)\n        predictions = map(lambda token: tagmap[token.tag_], doc)\n        actual = map(lambda tag: tagmap[tag], tags)\n        \n        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n        n_correct = len(correct_predictions)\n        correct += n_correct\n        total += len(words)\n        \n    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n    return result\n\ndef generate_tagmap():\n    def adjust_value(x):\n        if x == '.':\n            val = PUNCT\n        elif x=='PRT':\n            val = PART\n        else:\n            val = getattr(spacy.symbols, x)\n        return {POS:val}\n    nltk_map = tagset_mapping('en-brown','universal')\n    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n    return adj_map\n\nfeatures = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n,(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n,(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n,(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n,(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n\ntagmap = generate_tagmap()\nvocab = Vocab(tag_map = tagmap)\ntagger = Tagger(vocab)\n\n\npretraining_accuracy = validate(test, tagger)\nprint pretraining_accuracy\n\nfor i in range(10):\n    train_fold, test_fold = train_test_split(train, test_size = .2)\n    for words, tags in train_fold:\n        doc = Doc(vocab, words=words)\n        gold = GoldParse(doc, tags=tags)   \n        tagger.update(doc, gold)\n    current_accuracy = validate(test_fold, tagger)\n    print current_accuracy\n    np.random.shuffle(train)\ntagger.model.end_training()\n\nposttraining_accuracy = validate(test, tagger)\nprint posttraining_accuracy", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.symbols import *\nfrom spacy.language_data import TAG_MAP\nfrom spacy.vocab import Vocab\nfrom spacy.tagger import Tagger\nfrom spacy.tokens import Doc\nfrom spacy.gold import GoldParse\nfrom nltk.tag import tagset_mapping\nfrom spacy.en import English\nfrom spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\ndef generate_tagmap():\n    def adjust_value(x):\n        if x == '.':\n            val = PUNCT\n        elif x=='PRT':\n            val = PART\n        else:\n            val = getattr(spacy.symbols, x)\n        return {POS:val}\n    nltk_map = tagset_mapping('en-brown','universal')\n    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n    return adj_map\n\nfeatures = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n,(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n,(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n,(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n,(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n\ntagmap = generate_tagmap()", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "nlp.vocab.morphology.tag_map", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import spacy\nfrom spacy.syntax.arc_eager import ArcEager\nfrom spacy.syntax.ner import BiluoPushDown\nfrom spacy.tagger import Tagger\nfrom spacy.syntax.parser import Parser\nfrom spacy.syntax.nonproj import PseudoProjectivity\nfrom os import path\nimport shutil\n\ndef train(Language, train_loc, model_dir, n_iter=15, feat_set=u'ner', seed=0,\n      gold_preproc=False, n_sents=0):\n    print(\"Setup model dir\")\n    ner_model_dir = path.join(model_dir, 'ner')\n    if path.exists(ner_model_dir):\n        shutil.rmtree(ner_model_dir)\n    os.mkdir(ner_model_dir)\n    nlp = Language(path=model_dir, tagger=False, parser=False, entity=False)\n    labels = BiluoPushdown.get_labels(gold_tuples)\n    Config.write(ner_model_dir, 'config', features=feat_set, seed=seed,\n             labels=labels)\n    nlp.entity = Parser.from_dir(ner_model_dir)\n    nlp.tagger = Tagger.blank(nlp.vocab, Tagger.default_templates())\n    for itn in range(nr_iter):\n        for _, sents in gold_tuples:\n            for annot_tuples, _ in sents:\n                tokens = nlp.tokenizer.tokens_from_list(annot_tuples[1])\n                nlp.tagger.tag_from_strings(tokens, annot_tuples[2])\n                gold = GoldParse(tokens, annot_tuples)\n                loss += nlp.entity.train(tokens, gold)\n        random.shuffle(gold_tuples)\n    nlp.entity.model.end_training()\n    return nlp\n\n\ntrain(spacy.language.Language, 'train',str(p.path))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "str(p.path)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "map(lambda x: tagmap[x], train_fold[0][1])", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.tagger import Tagger\nvocab = Vocab(tag_map = tagmap)\n\n\n#doc = Doc(vocab, words = map(unicode,['i','like','green','eggs']))\ndoc = nlp.tokenizer(u\"I like green eggs\")\nvocab = nlp.vocab\ntagger1 = Tagger(vocab)\ntagger1(doc)\nmap(lambda x: (x.pos_, x.orth_), doc)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "nlp.tagger.feature_templates", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "doc = Doc(vocab, words = map(unicode,['i','like','green','eggs']))\ntagger(doc)\nmap(lambda x: (x.pos_, x.orth_), doc)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "d = Doc(vocab, words = test[0][0])\ntagger(d)\nmap(lambda x: (x.orth_, x.pos_, x.tag_) , d)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "def validate(test_data, tagger):\n    correct = 0\n    total = 0\n    \n    for words, tags in test_data:\n        doc = Doc(vocab, words=words)\n        tagger(doc)\n        predictions = map(lambda token: tagmap[token.tag_], doc)\n        actual = map(lambda tag: tagmap[tag], tags)\n        \n        correct_predictions = filter(lambda x: x[0] == x[1], zip(predictions, actual))\n        n_correct = len(correct_predictions)\n        correct += n_correct\n        total += len(words)\n        \n    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n    return result", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "toks = nlp.tokenizer.tokens_from_list(sents[0])", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "DATA1 = zip(*DATA)\nsents, tags = DATA1\nsents = [map(unicode, i) for i in sents]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "DATA[0][0]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "def map_to_unicode(list_of_strings):\n    return map(unicode, list_of_strings)\ndef make_docs(doc_tuples):\n    \n    return [nlp.tokenizer.tokens_from_list(map_to_unicode(sent_tuples[0]))\n                    for sent_tuples in doc_tuples]\ndocs = make_docs(DATA)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "for doc, tokens_tags_tuple in zip(docs, DATA):\n    tokens = tokens_tags_tuple[0]\n    tags = tokens_tags_tuple[1]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "nlp.train?", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "doc = docs[0]\ntoks_tags_tuple = DATA[0]\ntags = toks_tags_tuple[1]\nGoldParse.from_annot_tuples(doc, tags)\n           ", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "def make_golds(docs, doc_tuples):\n    return [GoldParse.from_annot_tuples(doc, tokens_tags_tuple[1])\n            for doc, tokens_tags_tuple in zip(docs, doc_tuples)]\ngolds = make_golds(docs, DATA)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "d = nlp.tokenizer.tokens_from_list([u'The',\n u'Fulton',\n u'County',\n u'Grand',\n u'Jury',\n u'said',\n u'Friday',\n u'an',\n u'investigation',\n u'of',\n u\"Atlanta's\",\n u'recent',\n u'primary',\n u'election',\n u'produced',\n u'``',\n u'no',\n u'evidence',\n u\"''\",\n u'that',\n u'any',\n u'irregularities',\n u'took',\n u'place',\n u'.'])\nd[0]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "GoldParse.from_annot_tuples()", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.gold import GoldParse\ndef make_golds(docs, paragraph_tuples):\n        if len(docs) == 1:\n            return [GoldParse.from_annot_tuples(docs[0], sent_tuples[0])\n                    for sent_tuples in paragraph_tuples]\n        else:\n            return [GoldParse.from_annot_tuples(doc, sent_tuples[0])\n                    for doc, sent_tuples in zip(docs, paragraph_tuples)]\n", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "from spacy.scorer import Scorer\ndef score_model(scorer, nlp, raw_text, annot_tuples):\n    if raw_text is None:\n        tokens = nlp.tokenizer.tokens_from_list(annot_tuples[1])\n    else:\n        tokens = nlp.tokenizer(raw_text)\n    nlp.tagger(tokens)\n    gold = GoldParse(tokens, annot_tuples)\n    scorer.score(tokens, gold)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "doc = nlp(u'this is a test')\nnlp.tagger.feature_templates", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "doc = Doc(vocab, words = [u'I',u'like',u'green',u'eggs',u'and',u'ham'])\n#doc = Doc(vocab, words = DATA[5][0])\ntagger(doc)\nmap(lambda x: (x.orth_, x.pos_, x.tag_), doc)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "DATA[5][0]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "tagger.vocab.morphology.tag_map", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.vocab import Vocab\nfrom spacy.tagger import Tagger\nfrom spacy.tokens import Doc\nfrom spacy.gold import GoldParse\nfrom thinc.linear import avgtron\nfrom __future__ import unicode_literals\nfrom spacy.symbols import *\nfrom numpy import random\nimport numpy as np\n\nTAG_MAP = {\n    \".\":        {POS: PUNCT, \"PunctType\": \"peri\"},\n    \",\":        {POS: PUNCT, \"PunctType\": \"comm\"},\n    \"-LRB-\":    {POS: PUNCT, \"PunctType\": \"brck\", \"PunctSide\": \"ini\"},\n    \"-RRB-\":    {POS: PUNCT, \"PunctType\": \"brck\", \"PunctSide\": \"fin\"},\n    \"``\":       {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"ini\"},\n    \"\\\"\\\"\":     {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"fin\"},\n    \"''\":       {POS: PUNCT, \"PunctType\": \"quot\", \"PunctSide\": \"fin\"},\n    \":\":        {POS: PUNCT},\n    \"(\":        {POS: PUNCT},\n    \")\":        {POS: PUNCT},\n    \"$\":        {POS: SYM, \"Other\": {\"SymType\": \"currency\"}},\n    \"#\":        {POS: SYM, \"Other\": {\"SymType\": \"numbersign\"}},\n    'ADJ':      {POS: ADJ},\n    'ADP':      {POS: ADP},\n    'ADV':      {POS: ADV},\n    \"AFX\":      {POS: ADJ,  \"Hyph\": \"yes\"},\n    'CONJ':      {POS: CONJ},\n    \"CC\":       {POS: CONJ, \"ConjType\": \"coor\"},\n    \"CD\":       {POS: NUM, \"NumType\": \"card\"},\n    \"DT\":       {POS: DET},\n    \"DET\":       {POS: DET},\n    \"EX\":       {POS: ADV, \"AdvType\": \"ex\"},\n    \"FW\":       {POS: X, \"Foreign\": \"yes\"},\n    \"HYPH\":     {POS: PUNCT, \"PunctType\": \"dash\"},\n    \"IN\":       {POS: ADP},\n    \"JJ\":       {POS: ADJ, \"Degree\": \"pos\"},\n    \"JJR\":      {POS: ADJ, \"Degree\": \"comp\"},\n    \"JJS\":      {POS: ADJ, \"Degree\": \"sup\"},\n    \"LS\":       {POS: PUNCT, \"NumType\": \"ord\"},\n    \"MD\":       {POS: VERB, \"VerbType\": \"mod\"},\n    \"NIL\":      {POS: \"\"},\n    \"NN\":       {POS: NOUN, \"Number\": \"sing\"},\n    \"NOUN\":     {POS: NOUN},\n    \"NNP\":      {POS: PROPN, \"NounType\": \"prop\", \"Number\": \"sing\"},\n    \"NNPS\":     {POS: PROPN, \"NounType\": \"prop\", \"Number\": \"plur\"},\n    \"NNS\":      {POS: NOUN, \"Number\": \"plur\"},\n    \"NUM\":      {POS: NUM},\n    \"PDT\":      {POS: ADJ, \"AdjType\": \"pdt\", \"PronType\": \"prn\"},\n    \"POS\":      {POS: PART, \"Poss\": \"yes\"},\n    \"PRON\":     {POS: PRON},\n    \"PRP\":      {POS: PRON, \"PronType\": \"prs\"},\n    \"PRP$\":     {POS: ADJ, \"PronType\": \"prs\", \"Poss\": \"yes\"},\n    'PRT':      {POS: PART},\n    \"RB\":       {POS: ADV, \"Degree\": \"pos\"},\n    \"RBR\":      {POS: ADV, \"Degree\": \"comp\"},\n    \"RBS\":      {POS: ADV, \"Degree\": \"sup\"},\n    \"RP\":       {POS: PART},\n    \"SYM\":      {POS: SYM},\n    \"TO\":       {POS: PART, \"PartType\": \"inf\", \"VerbForm\": \"inf\"},\n    \"UH\":       {POS: INTJ},\n    \"VB\":       {POS: VERB, \"VerbForm\": \"inf\"},\n    \"VERB\":     {POS: VERB},\n    \"VBD\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"past\"},\n    \"VBG\":      {POS: VERB, \"VerbForm\": \"part\", \"Tense\": \"pres\", \"Aspect\": \"prog\"},\n    \"VBN\":      {POS: VERB, \"VerbForm\": \"part\", \"Tense\": \"past\", \"Aspect\": \"perf\"},\n    \"VBP\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"pres\"},\n    \"VBZ\":      {POS: VERB, \"VerbForm\": \"fin\", \"Tense\": \"pres\", \"Number\": \"sing\", \"Person\": 3},\n    \"WDT\":      {POS: ADJ, \"PronType\": \"int|rel\"},\n    \"WP\":       {POS: NOUN, \"PronType\": \"int|rel\"},\n    \"WP$\":      {POS: ADJ, \"Poss\": \"yes\", \"PronType\": \"int|rel\"},\n    \"WRB\":      {POS: ADV, \"PronType\": \"int|rel\"},\n    \"SP\":       {POS: SPACE},\n    \"ADD\":      {POS: X},\n    \"NFP\":      {POS: PUNCT},\n    \"GW\":       {POS: X},\n    \"XX\":       {POS: X},\n    \"BES\":      {POS: VERB},\n    \"HVS\":      {POS: VERB},\n    \"X\":        {POS:X}\n}\n\ncorpus = nltk_corpus('brown')\nDATA = np.array(corpus.tagged_sents(tagset='universal'))\nvocab = Vocab(tag_map = TAG_MAP)\ntagger = Tagger(vocab)\n# for i in range(25):\n#     print i\n#     for sent in DATA:\n#         words, tags = zip(*sent)\n#         doc = Doc(vocab, words = words)\n#         gold = GoldParse(doc, tags=tags)\n#         tagger.update(doc, gold)\n#     random.shuffle(DATA)\n# tagger.model.end_training()\n\n", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "#question answering part 1\ndef merge_PROPN_chunk(matcher, doc, i, matches):\n    if i != len(matches)-1:\n        return None\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge() \n        \nPROPN_PATTERN = [\n            {POS:u'PROPN','OP':'+'}   \n          ]\n    \n\nmatcher = Matcher(nlp.vocab)\nmatcher.add_entity(u'PROPN_CHUNK', on_match = merge_PROPN_chunk)\nmatcher.add_pattern(u\"PROPN_CHUNK\", PROPN_PATTERN,  label=u'PROPN_CHUNK')\n\ndef merge_ents(doc):\n    for ent in doc.ents:\n        ent.merge()\n\ndef post_process(doc):\n    matcher(doc)\n    merge_ents(doc)\n    return doc\n\n\n\ndef get_answer_type(doc):\n    post_process(doc)\n    tags = ['WDT','WP','WRB']\n    \n    question_type_flags = filter(lambda token: token.tag_ in tags, doc)\n    \n    if len(question_type_flags)==0:\n        warn(\"No question tag was found in request\")\n        return False\n    \n    elif len(question_type_flags) > 1:\n        warn(\"Multiple question tags found in request, resolving using parse\")\n        return False\n    \n    else:\n        tag = question_type_flags[0]\n        answer_requirements = get_answer_requirements(tag)\n        return answer_requirements\n    \ndef get_answer_requirements(token):\n    if token.tag_ == 'WRB':\n        if token.lower_ == 'where':\n            #Where was Star Wars Filmed\n            return ['LOCATION']\n        elif token.lower_ == 'when':\n            #When was Star Wars Filmed\n            return ['DATE']\n        elif token.lower_ == 'how':\n            #How much did Star Wars make?\n            if token.nbor().lower_ in ('much', 'many'):\n                return ['QUANTITY']\n\n            #How old is star wars?\n            elif token.nbor().lower_ in ('long', 'old'):\n                return ['DURATION']\n            else:\n                return False\n        elif token.lower() == 'whom':\n            #Whom did you see?\n            return ['PERSON','ORG']      \n        else:\n            return False\n    elif token.tag_ == 'WP':\n        #Asking for Identity\n        if token.lower_ in ('who', 'whose'):\n            #Who directed Star Wars?\n            return ['PERSON','ORG']\n        if token.lower_ in ('which','what'):\n            #What is Star Wars\n            return False \n        else: \n            return False\n    elif token.tag_ == 'WDT':\n        #asking for a choice among options\n        if token.lower_ in ('which','what'):\n            #which Star Wars did you like best?\n            return [token.nbor().lower_] #return neighbor\n        else:\n            return False\n    else:\n        return False    \n    \n    \ndef info(string):\n    return map(lambda x: (x.orth_, x.tag_, x.pos_), nlp(unicode(string)))\n\nget_answer_type(nlp(u'When was Star Wars released?'))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "def nltk_reader(corpus_name, limit = None):\n    corpus = getattr(nltk.corpus, corpus_name)\n    try:\n        corpus.ensure_loaded()\n    except:\n        nltk.download(corpus_name)\n    fileids = corpus.fileids()\n    if limit:\n        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n    else:\n        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n    return doc_iter", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "import spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS\nimport zipfile\nfrom ds_api.cache_utils import load_cache, s3_key\nfrom spacy.en import English\n\ndef merge_PROPN_chunk(matcher, doc, i, matches):\n    if i != len(matches)-1:\n        return None\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge() \n\ndef custom_matcher(doc):\n    matcher = Matcher(doc.vocab)\n    matcher.add_entity(u'PROPN_CHUNK', on_match=merge_PROPN_chunk)\n    matcher.add_pattern(u\"PROPN_CHUNK\", verb_pattern,  label=u'PROPN_CHUNK')\n    return matcher(doc) \n\nPROPN_PATTERN = [\n            {POS:u'NN', 'OP':'*'},\n            {POS:u'NNP', 'OP':'*'},\n            {POS:u'PROPN','OP':'+'},\n            {POS:u'NN', 'OP':'*'},\n            {POS:u'NNP', 'OP':'*'}    \n          ]\n       \n        \npath = 'spacy-resources/spacy_files/en-1.1.0/'\nif os.path.exists(path):\n    nlp = spacy.en.English(path=path, create_pipeline = create_pipeline)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from spacy.attrs import POS\nfrom spacy.matcher import Matcher\nreuters = nltk_reader('abc', limit=100)\n\ndef merge_PROPN_chunk(matcher, doc, i, matches):\n    if i != len(matches)-1:\n        return None\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge() \n\nPROPN_PATTERN = [\n            {POS:u'PROPN','OP':'+'}\n          ]\n\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add_entity(u'PROPN_CHUNK', on_match = merge_PROPN_chunk)\nmatcher.add_pattern(u\"PROPN_CHUNK\", PROPN_PATTERN,  label=u'PROPN_CHUNK')\n\n\nproper_nouns = Counter()\n\nfor doc in nlp.pipe(reuters, n_threads=4):\n    match_idx = matcher(doc)\n    matches = map(lambda x: doc[x[2]:x[3]].orth_, match_idx)\n    proper_nouns.update(matches)", "execution_count": null, "cell_type": "code", "metadata": {"_datascience": {}, "scrolled": false, "collapsed": false}}, {"outputs": [], "source": "original_text = u'I was able to find the paper by using the Google search engine.'\nmangled_text = u'I was goble to find the doomple by greping the Dongle search engine.'", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "#grab sentences from SpaCy\noriginal_sentence = list(nlp(original_text).sents)[0]\nmangled_sentence = list(nlp(mangled_text).sents)[0]\n\n\n\nparts_of_speech = {}\n\n#iterate over each sentence, grabbing the tagged POS\nfor idx in range(len(original_sentence)):\n    original_word, mangled_word = original_sentence[idx], mangled_sentence[idx]\n    \n    record = {\n             'original word': original_word\n             , 'mangled word': mangled_word\n             , 'original pos':original_word.pos_\n             , 'mangled pos':mangled_word.pos_\n    }\n    \n    parts_of_speech[idx] = record\n    \n    \n#present results in a table\ncolumns = ['original word','mangled word','original pos', 'mangled pos']\npd.DataFrame(parts_of_speech).T[columns]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "#this example uses shows the frequency of tokens with multiple\n#parts of speech\n\n#grab the brown corpus\nfileids = brown.fileids()\ndoc_iter = (\" \".join([\" \".join(j) for j in brown.sents(fileid)]) for fileid in fileids)\n\n#create a dictionary of sets\nd = defaultdict(set)\n\nfor doc in nlp.pipe(doc_iter, n_threads=4):\n    for token in doc:\n        #add the token's part of speech to the lexeme's set\n        d[token.lemma_].add(token.pos_)\n\n#grab the number of unique POSs per lexeme\nlengths = map(lambda key: len(d[key]), d) \nc = Counter(lengths)\nN = float(sum(c.values()))\n\n#plot\nax = pd.Series(c).map(lambda x: x / N).plot(kind = 'bar')\nax.set_xlabel(\"Unique Parts of Speech in Brown Corpus\")\nax.set_ylabel(\"Percent of Unique Words in Vocabulary\")\nax.grid(axis='x')\nax.set_title(\"20% of Tokens in Brown Corpus Have Multiple Unique POS\")\nplt.show()", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "print d['down']", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "def merge_phrases(matcher, doc, i, matches):\n    '''\n    Merge a phrase. We have to be careful here because we'll change the token indices.\n    To avoid problems, merge all the phrases once we're called on the last match.\n    '''\n    if i != len(matches)-1:\n        return None\n    # Get Span objects\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n\nmatcher.add_entity('VERB_PHRASE', on_match=merge_phrases)\nmatcher.add_pattern('VERB_PHRASE', verb_pattern)\n# doc = Doc(matcher.vocab, words=[u'Google', u'Now', u'is', u'being', u'rebranded'])\n# matcher(doc)\n# print([w.text for w in doc])", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "spacy.__version__", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS\nimport zipfile\nfrom ds_api.cache_utils import load_cache, s3_key\nfrom spacy.en import English\n\ndef merge_verb_chunk(matcher, doc, i, matches):\n    if i != len(matches)-1:\n        return None\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge() \n\ndef custom_matcher(doc):\n    matcher = Matcher(doc.vocab)\n    matcher.add_entity(u'VERB_CHUNK', on_match=merge_verb_chunk)\n    matcher.add_pattern(u\"VERB_CHUNK\", verb_pattern,  label=u'VERB_CHUNK')\n    return matcher(doc) \n        \ndef create_pipeline(nlp):\n    return (nlp.tagger, nlp.parser, nlp.matcher, nlp.entity, custom_matcher, custom_matcher)\n\nverb_pattern = [\n            {POS:u'AUX', 'OP':'*'},\n            {POS:u'ADV', 'OP':'*'},\n            {POS:u'VERB','OP':'+'}\n          ]\n       \n        \npath = 'spacy-resources/spacy_files/en-1.1.0/'\nif os.path.exists(path):\n    nlp = spacy.en.English(path=path, create_pipeline = create_pipeline)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "text = u'I could hardly believe that the Google search engine was that good'\ndoc = nlp(text)\nlist(doc.ents)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS\nimport zipfile\nfrom ds_api.cache_utils import load_cache, s3_key\nfrom spacy.en import English\ndef get_spacy_parser(out_dir = 'spacy-resources'):\n    load_cache('spacy-english.zip', 'spacy-english.zip')\n    path_to_zip_file = 'spacy-english.zip'\n    zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n    zip_ref.extractall(out_dir)\n    zip_ref.close()    \n    english_directory = '{}/spacy_files/en-1.1.0/'.format(out_dir)\n    parser = English(path=english_directory)\n    return parser\nif os.path.exists('spacy-resources/spacy_files/en-1.1.0/')\n\n\ndef merge_verb_phrase(matcher, doc, i, matches):\n    '''\n    Merge a phrase. We have to be careful here because we'll change the token indices.\n    To avoid problems, merge all the phrases once we're called on the last match.\n    '''\n    if i != len(matches)-1:\n        return None\n    # Get Span objects\n    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n    for ent_id, label, span in spans:\n        span.merge(label_=label, pos='VERB')\n        \nverb_pattern = [\n            {POS:'AUX', 'OP':'*'},\n            {POS:'ADV', 'OP':'*'},\n            {POS:'VERB','OP':'+'},\n            {POS:'AUX', 'OP':'*'},\n            {POS:'ADV', 'OP':'*'},\n            {POS:'VERB','OP':'*'}\n          ]\n\ndef custom_matcher(doc):\n    matcher = Matcher(doc.vocab)\n    matcher.add_entity('VERB_PHRASE', on_match=merge_verb_phrase)\n    matcher.add_pattern(\"VERB_PHRASE\", verb_pattern,  label='VERB_PHRASE')\n    return matcher(doc)\n\n\n\n#matcher = Matcher(nlp.vocab)\n#matcher.add_entity('VERB_PHRASE', on_match=merge_verb_phrase)\n#matcher.add_pattern(\"VERB_PHRASE\", verb_pattern,  label='VERB_PHRASE')\n\ndef merge_texts(question):\n    for chunk in list(question.noun_chunks):\n        chunk.merge()\n    for ent in list(question.ents):\n        ent.merge()\n\ndoc = nlp(u'I could hardly believe it')\nmatches = matcher(doc)\n\nprint map(lambda x: x.pos_, doc)\n\nprint\nprint list(doc.ents)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import textacy", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "d = nlp(u'You would tell me if you could help me')\nprint map(lambda x: x.pos_, d)\nprint\nmatcher(d)\n\n\nprint map(lambda x: x.pos_, d)\nprint\nprint list(d.ents)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "spacy.en.English", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import zipfile\nfrom ds_api.cache_utils import load_cache, s3_key\nfrom spacy.en import English\ndef get_spacy_parser(out_dir = 'spacy-resources'):\n    load_cache('spacy-english.zip', 'spacy-english.zip')\n    path_to_zip_file = 'spacy-english.zip'\n    zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n    zip_ref.extractall(out_dir)\n    zip_ref.close()    \n    english_directory = '{}/spacy_files/en-1.1.0/'.format(out_dir)\n    parser = English(path=english_directory)\n    return parser\n\n\nnlp = get_spacy_parser()", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from nltk.corpus import conll2000\nnltk.download('conll2000')\ntrain_sents = conll2000.chunked_sents('train.txt', chunk_types=['VP'])", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "s = train_sents[0]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "!sudo pip install spacy -U", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "fileids = conll2000.fileids()", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}, {"outputs": [], "source": "fileids[1]", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "import io\nimport requests\n#https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv\nurl = 'https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv?token=AGz3FdVBO7kTQsMgkoMrqdZlTI1Tbfmiks5YXX67wA%3D%3D'\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "c", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "zip(words, tags)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "tagger2 = Tagger(vocab)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "\ndoc = nlp(u'I am bob how are you?')\ntagger2(doc)\nmap(lambda x: x.pos_, doc)", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"_datascience": {}, "scrolled": true, "collapsed": false}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "from nltk.corpus import w", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "!sudo pip install cufflinks", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [{"output_type": "stream", "name": "stdout", "text": "SECRET_ENV_GITHUB_USERNAME=johnchuckcase\r\nSECRET_ENV_TEST_ENV=test\r\nSECRET_ENV_AWS_ACCESS_KEY_SHARED=AKIAJ4Z2PMTE5XZJ3SLQ\r\nSECRET_ENV_REDSHIFT_PASS=&$&CAEcI$l3p@Xtr\r\nSECRET_ENV_AWS_ACCESS_KEY_EBTH=AKIAJAHDMEITUE6GQD7Q\r\nSECRET_ENV_CUSTOMER_EBTH=ebth\r\nSECRET_ENV_DS_EBTH_RS_PASSWORD=O630ABBKfFGbZub@\r\nSECRET_ENV_REDSHIFT_HOST=data-science.cex3rfvdw0wv.us-west-2.redshift.amazonaws.com\r\nSECRET_ENV_DS_EBTH_RS_HOST=data-science.cex3rfvdw0wv.us-west-2.redshift.amazonaws.com\r\nSECRET_ENV_REDSHIFT_USER=nnadkarni\r\nSECRET_ENV_CUSTOMER=internal\r\nSECRET_ENV_DS_EBTH_RS_USER=jrgauthier\r\nSECRET_ENV_GITHUB_PASSWORD=green1994\r\nSECRET_ENV_REDSHIFT_PORT=5439\r\nSECRET_ENV_AWS_SECRET_KEY_SHARED=XDu7pB+dzf1iA821dtThMIl7GY4yHwT24vHjYZ8O\r\nSECRET_ENV_DS_EBTH_RS_PORT=5439\r\nSECRET_ENV_AWS_SECRET_KEY_EBTH=x2ertIFJBA+unDvCKqEGxC5WPnx6tkpzi4iSFdVL\r\n"}], "source": "SECRET_ENV_AARON_PLOTLY_USERNAME = 'aikramer2'\nSECRET_ENV_AARON_PLOT_API_KEY = 'pQ4lbj8qWkPklCcowaHZ'", "execution_count": 2, "cell_type": "code", "metadata": {"collapsed": false, "_datascience": {}}}, {"outputs": [], "source": "SECRET_ENV_AARON_PLOTLY_USERNAME", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_datascience": {}}}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "codemirror_mode": {"name": "ipython", "version": 2}, "version": "2.7.12", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python"}, "_datascience": {"notebookId": 749}, "kernelspec": {"language": "python", "name": "python2", "display_name": "Python 2"}}, "nbformat_minor": 1}